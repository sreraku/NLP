{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jN7tT2jvuYxo"
      },
      "source": [
        "# Toxicity classification using BERT\n",
        "\n",
        "**Description:** This notebook builds a classification model by fine tuning BERT to label comments with 6 classes 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'.\n",
        "\n",
        "The data used for training the model was originally sourced from [Kaggle Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge). \n",
        "\n",
        "<a id = 'returnToTop'></a>\n",
        "\n",
        "## Notebook Contents\n",
        "  * 1. [Setup](#setup) \n",
        "  * 2. [Data](#data)  \n",
        "  * 3. [Tokenization](#tokenization)\n",
        "  * 4. [Model Training](#training)\n",
        "  * 5. [Model Evaluation](#evaluation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knuH9pWDuYxs"
      },
      "source": [
        "\n",
        "<a id = 'setup'></a>\n",
        "\n",
        "## 1. Setup\n",
        "\n",
        "Install required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KMdabVGlPoqk"
      },
      "outputs": [],
      "source": [
        "!pip install transformers==4.27.2 --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall tensorflow --yes\n",
        "!pip install tensorflow==2.11.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQnEmtG7vvuB",
        "outputId": "96b2959f-5dfd-411d-edd2-6d54a01927fa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.11.0\n",
            "Uninstalling tensorflow-2.11.0:\n",
            "  Successfully uninstalled tensorflow-2.11.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.11.0\n",
            "  Using cached tensorflow-2.11.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (3.8.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (23.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (1.6.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (1.53.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (1.14.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (1.22.4)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (3.19.6)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (0.32.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (1.16.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (2.11.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (3.3.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (16.0.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (2.2.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (0.4.0)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (2.11.2)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (2.11.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (23.3.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (0.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (67.6.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.11.0) (1.4.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow==2.11.0) (0.40.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.17.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.27.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.2.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (6.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.2.2)\n",
            "Installing collected packages: tensorflow\n",
            "Successfully installed tensorflow-2.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEuJaUNnuYxw"
      },
      "source": [
        "Import required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tB-wHngguYxx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Zn-icS7sP5pj"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, TFBertModel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "print(transformers.__version__)\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bQU8l4asvI4",
        "outputId": "fb14fee6-e9f8-40ef-b10f-89a3a8a14c65"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.27.2\n",
            "2.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVV7cXFjuYx4"
      },
      "source": [
        "[Return to Top](#returnToTop)  \n",
        "<a id = 'data'></a>\n",
        "\n",
        "## 2. Data\n",
        "\n",
        "The jigsaw database has been downloaded from kaggle, cleaned and preprocessed and split into train, validation and test datasets. The datsets are stored on amazon S3 where we will be accessing them from."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LOAD_TEST_DATA = False"
      ],
      "metadata": {
        "id": "vkJKfKXqy2LE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if (LOAD_TEST_DATA):\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "  df_train = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/w266project/sample_train_data.csv\")\n",
        "  df_valid = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/w266project/sample_validation_data.csv\")\n",
        "  df_test = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/w266project/sample_test_data.csv\")\n",
        "  \n",
        "else:\n",
        "  df_train = pd.read_csv(\"https://adamhyman-public.s3.amazonaws.com/w266/for_modeling/train_data.csv\")\n",
        "\n",
        "  df_valid = pd.read_csv(\"https://adamhyman-public.s3.amazonaws.com/w266/for_modeling/validation_data.csv\")\n",
        "\n",
        "  df_test = pd.read_csv(\"https://adamhyman-public.s3.amazonaws.com/w266/for_modeling/test_data.csv\")"
      ],
      "metadata": {
        "id": "8DyWGY9j7OVW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LPfnIOvf7k3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Mp54r0vgEwyg",
        "outputId": "f58d2ca7-07a8-480f-835a-6a1cb38710ce"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 id                                       comment_text  toxic  \\\n",
              "0  1f248a9d4e6a747e                   unsigned comment by userjohn4890      0   \n",
              "1  4cb60854285a70b8   how can you block me when you are just an editor      0   \n",
              "2  4462a926cf64b036    berwickshire i have added the category berwi...      0   \n",
              "3  68d5bbcc7bdf32e3  struggling with format issues  constant interr...      0   \n",
              "4  d8869e230033c100  the article team kman has been speedily delete...      0   \n",
              "\n",
              "   severe_toxic  obscene  threat  insult  identity_hate  \n",
              "0             0        0       0       0              0  \n",
              "1             0        0       0       0              0  \n",
              "2             0        0       0       0              0  \n",
              "3             0        0       0       0              0  \n",
              "4             0        0       0       0              0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4a0e29bb-a932-4a9b-93a2-7a80ee5b5df7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1f248a9d4e6a747e</td>\n",
              "      <td>unsigned comment by userjohn4890</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4cb60854285a70b8</td>\n",
              "      <td>how can you block me when you are just an editor</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4462a926cf64b036</td>\n",
              "      <td>berwickshire i have added the category berwi...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>68d5bbcc7bdf32e3</td>\n",
              "      <td>struggling with format issues  constant interr...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>d8869e230033c100</td>\n",
              "      <td>the article team kman has been speedily delete...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4a0e29bb-a932-4a9b-93a2-7a80ee5b5df7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4a0e29bb-a932-4a9b-93a2-7a80ee5b5df7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4a0e29bb-a932-4a9b-93a2-7a80ee5b5df7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = df_test.dropna(how='any',axis=0) "
      ],
      "metadata": {
        "id": "M6pnH0jP8aR4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#split input and output variables\n",
        "train_comments, train_labels = df_train[\"comment_text\"], df_train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n",
        "valid_comments, valid_labels = df_valid[\"comment_text\"], df_valid[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n",
        "test_comments, test_labels = df_test[\"comment_text\"], df_test[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]"
      ],
      "metadata": {
        "id": "tfNu44aMESnE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "hVXmgm7YuYx4"
      },
      "outputs": [],
      "source": [
        "#covert to tensors\n",
        "train_comments, train_labels = tf.convert_to_tensor(train_comments), tf.convert_to_tensor(train_labels)\n",
        "valid_comments, valid_labels = tf.convert_to_tensor(valid_comments), tf.convert_to_tensor(valid_labels)\n",
        "test_comments, test_labels = tf.convert_to_tensor(test_comments), tf.convert_to_tensor(test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bz2tFw9VuYx4",
        "outputId": "fc9e47c5-c294-477c-c69d-5d9091cc0c73"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(4,), dtype=string, numpy=\n",
              "array([b'unsigned comment by userjohn4890',\n",
              "       b'how can you block me when you are just an editor',\n",
              "       b'  berwickshire i have added the category berwickshire i hope people do not mind this i realise it is no longer considered to be in berwickshire but it is relevant to it for several reasons including the fact that the county is named after the town  ',\n",
              "       b'struggling with format issues  constant interruptions it is interesting this way  see what you think thanks for your help'],\n",
              "      dtype=object)>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "#verify input data\n",
        "train_comments[:4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fD5Ah2muYx5",
        "outputId": "a67ee4e5-91f7-41a6-e53c-8b618dd0a0f7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(4, 6), dtype=int64, numpy=\n",
              "array([[0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0]])>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "#verify outpit labels\n",
        "train_labels[:4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyUVciGOQ72E"
      },
      "source": [
        "[Return to Top](#returnToTop)  \n",
        "<a id = 'tokenization'></a>\n",
        "## 3. Tokenization\n",
        "\n",
        "Get the pre-trained BERT model and tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VvVL8deRhtY",
        "outputId": "55a7c40a-e655-4736-ef29-9728f7190159"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "bert_model = TFBertModel.from_pretrained('bert-base-cased')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT Tokenization of training and validation data\n",
        "MAX_SEQUENCE_LENGTH = 128                 # set max_length of the input sequence\n",
        "\n",
        "train_examples = [x.decode('utf-8') for x in train_comments.numpy()]\n",
        "valid_examples = [x.decode('utf-8') for x in valid_comments.numpy()]\n",
        "\n",
        "x_train = bert_tokenizer(train_examples,\n",
        "              max_length=MAX_SEQUENCE_LENGTH,\n",
        "              truncation=True,\n",
        "              padding='max_length', \n",
        "              return_tensors='tf')\n",
        "y_train = train_labels\n",
        "\n",
        "x_valid = bert_tokenizer(valid_examples,\n",
        "              max_length=MAX_SEQUENCE_LENGTH,\n",
        "              truncation=True,\n",
        "              padding='max_length', \n",
        "              return_tensors='tf')\n",
        "y_valid = valid_labels"
      ],
      "metadata": {
        "id": "9SWuHpUtFe3o"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaJkBucTXDpY"
      },
      "source": [
        "[Return to Top](#returnToTop)  \n",
        "<a id = 'model'></a>\n",
        "\n",
        "# 4. Model definition and training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQwas6gme9Ts"
      },
      "source": [
        "Define the model..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "1RShlI4_V_JU"
      },
      "outputs": [],
      "source": [
        "def create_bert_classification_model(bert_model,\n",
        "                                     num_train_layers=0,\n",
        "                                     hidden_size=200, \n",
        "                                     dropout=0.3,\n",
        "                                     learning_rate=0.00005):\n",
        "    \"\"\"\n",
        "    Build a simple classification model with BERT. Use the Pooler Output for classification purposes\n",
        "    \"\"\"\n",
        "    if num_train_layers == 0:\n",
        "        # Freeze all layers of pre-trained BERT model\n",
        "        bert_model.trainable = False\n",
        "\n",
        "    elif num_train_layers == 12: \n",
        "        # Train all layers of the BERT model\n",
        "        bert_model.trainable = True\n",
        "\n",
        "    else:\n",
        "        # Restrict training to the num_train_layers outer transformer layers\n",
        "        retrain_layers = []\n",
        "\n",
        "        for retrain_layer_number in range(num_train_layers):\n",
        "\n",
        "            layer_code = '_' + str(11 - retrain_layer_number)\n",
        "            retrain_layers.append(layer_code)\n",
        "          \n",
        "        \n",
        "        print('retrain layers: ', retrain_layers)\n",
        "\n",
        "        for w in bert_model.weights:\n",
        "            if not any([x in w.name for x in retrain_layers]):\n",
        "                #print('freezing: ', w)\n",
        "                w._trainable = False\n",
        "\n",
        "    input_ids = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int64, name='input_ids_layer')\n",
        "    token_type_ids = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int64, name='token_type_ids_layer')\n",
        "    attention_mask = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int64, name='attention_mask_layer')\n",
        "\n",
        "    bert_inputs = {'input_ids': input_ids,\n",
        "                   'token_type_ids': token_type_ids,\n",
        "                   'attention_mask': attention_mask}      \n",
        "\n",
        "    bert_out = bert_model(bert_inputs)\n",
        "\n",
        "    pooler_token = bert_out[1]\n",
        "    #cls_token = bert_out[0][:, 0, :]\n",
        "\n",
        "    hidden = tf.keras.layers.Dense(hidden_size, activation='relu', name='hidden_layer')(pooler_token)\n",
        "\n",
        "    hidden = tf.keras.layers.Dropout(dropout)(hidden)  \n",
        "\n",
        "    classification = tf.keras.layers.Dense(6, activation='sigmoid',name='classification_layer')(hidden)\n",
        "    \n",
        "    classification_model = tf.keras.Model(inputs=[input_ids, token_type_ids, attention_mask], outputs=[classification])\n",
        "    \n",
        "    classification_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                                 loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), \n",
        "                                 metrics='accuracy')\n",
        "    \n",
        "    return classification_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "PbFsTmdPuYx6"
      },
      "outputs": [],
      "source": [
        "bert_classification_model = create_bert_classification_model(bert_model, num_train_layers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tk0Cs2GnoJZ",
        "outputId": "c5cc8521-e3a1-4211-fdfc-517e87c55052"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " attention_mask_layer (InputLay  [(None, 128)]       0           []                               \n",
            " er)                                                                                              \n",
            "                                                                                                  \n",
            " input_ids_layer (InputLayer)   [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " token_type_ids_layer (InputLay  [(None, 128)]       0           []                               \n",
            " er)                                                                                              \n",
            "                                                                                                  \n",
            " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  108310272   ['attention_mask_layer[0][0]',   \n",
            "                                thPoolingAndCrossAt               'input_ids_layer[0][0]',        \n",
            "                                tentions(last_hidde               'token_type_ids_layer[0][0]']   \n",
            "                                n_state=(None, 128,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " hidden_layer (Dense)           (None, 200)          153800      ['tf_bert_model[0][1]']          \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 200)          0           ['hidden_layer[0][0]']           \n",
            "                                                                                                  \n",
            " classification_layer (Dense)   (None, 6)            1206        ['dropout_37[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 108,465,278\n",
            "Trainable params: 155,006\n",
            "Non-trainable params: 108,310,272\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#confirm all layers are frozen\n",
        "bert_classification_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTQnDuypuYyD",
        "outputId": "1556b67b-15f1-428e-d700-c8b03f5e1c16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "3990/3990 [==============================] - 1499s 372ms/step - loss: 0.1323 - accuracy: 0.6707 - val_loss: 0.1054 - val_accuracy: 0.9941\n",
            "Epoch 2/2\n",
            "3990/3990 [==============================] - 1433s 359ms/step - loss: 0.1040 - accuracy: 0.8327 - val_loss: 0.0909 - val_accuracy: 0.9936\n"
          ]
        }
      ],
      "source": [
        "bert_classification_model_history = bert_classification_model.fit(\n",
        "    [x_train.input_ids, x_train.token_type_ids, x_train.attention_mask],\n",
        "    y_train,\n",
        "    validation_data=([x_valid.input_ids, x_valid.token_type_ids, x_valid.attention_mask], y_valid),\n",
        "    batch_size=32,\n",
        "    epochs=2\n",
        ")  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Return to Top](#returnToTop)  \n",
        "<a id = 'evaluation'></a>\n",
        "\n",
        "# 4. Model Evaluation"
      ],
      "metadata": {
        "id": "NqiKKjsKKYPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Run some tests\n",
        "test_commment = ['what a stupid useless creature']\n",
        "test_tokens = bert_tokenizer(test_commment,\n",
        "              max_length=MAX_SEQUENCE_LENGTH,\n",
        "              truncation=True,\n",
        "              padding='max_length', \n",
        "              return_tensors='tf')\n",
        "\n",
        "test_predictions = bert_classification_model.predict([test_tokens.input_ids, test_tokens.token_type_ids, test_tokens.attention_mask], batch_size=32)\n",
        "test_pred = np.where(test_predictions>=0.5, 1, 0)\n",
        "target_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "\n",
        "pred_df = pd.DataFrame(data = test_pred, columns = target_names)\n",
        "print(pred_df)"
      ],
      "metadata": {
        "id": "Cy9uOX-6L2Td",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60912ddf-2f39-499b-ed0e-9995b5e009b5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 3s 3s/step\n",
            "   toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
            "0      1             0        0       0       0              0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prepare test data\n",
        "test_examples = [x.decode('utf-8') for x in test_comments.numpy()]\n",
        "\n",
        "x_test = bert_tokenizer(test_examples,\n",
        "              max_length=MAX_SEQUENCE_LENGTH,\n",
        "              truncation=True,\n",
        "              padding='max_length', \n",
        "              return_tensors='tf')\n",
        "y_test = test_labels"
      ],
      "metadata": {
        "id": "Lf2WeT1cNpxZ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_examples[:10]"
      ],
      "metadata": {
        "id": "MrdATmrg_f9B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92d03966-3ae4-40a2-850b-a59dae3d85dc"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['thank you for understanding i think very highly of you and would not revert without discussion',\n",
              " 'dear god this site is horrible',\n",
              " ' somebody will invariably try to add religion  really  you mean the way people have invariably kept adding religion to the samuel beckett infobox  and why do you bother bringing up the longdead completely nonexistent influences issue  you are just flailing making up crap on the fly    for comparison the only explicit acknowledgement in the entire amos oz article that he is personally jewish is in the categories       ',\n",
              " '    it says it right there that it is a type the type of institution is needed in this case because there are three levels of suny schools   university centers and doctoral granting institutions   state colleges   community colleges    it is needed in this case to clarify that ub is a suny center it says it even in binghamton university university at albany state university of new york and stony brook university stop trying to say it is not because i am totally right in this case',\n",
              " '     before adding a new product to the list make sure it is relevant     before adding a new product to the list make sure it has a wikipedia entry already proving it is relevance and giving the reader the possibility to read more about it   otherwise it could be subject to deletion see this articles revision history',\n",
              " 'this other one from 1897',\n",
              " ' reason for banning throwing     this article needs a section on why throwing is banned at the moment to a noncricket fan it seems kind of arbitrary',\n",
              " 'blocked from editing wikipedia   ',\n",
              " ' arabs are committing genocide in iraq but no protests in europe     may europe also burn in hell',\n",
              " 'please stop if you continue to vandalize wikipedia as you did to homosexuality you will be blocked from editing']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# run the trained model on the test data (the model outputs probabilities)\n",
        "#y_test_predictions = bert_classification_model(x_test)\n",
        "y_test_predictions = bert_classification_model.predict([x_test.input_ids, x_test.token_type_ids, x_test.attention_mask], batch_size=32)\n",
        "\n",
        "# apply the threshold function to create a 0, 1 outcome\n",
        "y_test_pred = np.where(y_test_predictions>=0.5, 1, 0)\n",
        "y_test_pred[:10] # first 10 only"
      ],
      "metadata": {
        "id": "cWyzTdw6KukB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bbb2929-fba5-4b36-d548-e2e89d7ef1e5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2000/2000 [==============================] - 538s 269ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, y_test_pred, target_names=['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'], zero_division = 0))"
      ],
      "metadata": {
        "id": "UkBQvOeWLczk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0926a32-c975-4e97-a574-d3ae5a81864b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               precision    recall  f1-score   support\n",
            "\n",
            "        toxic       0.69      0.31      0.43      6090\n",
            " severe_toxic       0.00      0.00      0.00       367\n",
            "      obscene       0.73      0.24      0.36      3691\n",
            "       threat       0.00      0.00      0.00       211\n",
            "       insult       0.70      0.21      0.32      3427\n",
            "identity_hate       0.00      0.00      0.00       712\n",
            "\n",
            "    micro avg       0.70      0.24      0.36     14498\n",
            "    macro avg       0.35      0.13      0.18     14498\n",
            " weighted avg       0.64      0.24      0.35     14498\n",
            "  samples avg       0.03      0.02      0.02     14498\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import hamming_loss\n",
        "hamming_loss(y_test, y_test_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMu5mRmpGGaa",
        "outputId": "fb8699ce-050c-488d-db40-5a345728f932"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.03250901436044936"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #save the trained model\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# saved_model_path = ('/content/drive/My Drive/Colab Notebooks/w266project/bert_classifier')\n",
        "# bert_classification_model.save(saved_model_path, include_optimizer=False)"
      ],
      "metadata": {
        "id": "R5lZOsYxBMIO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f56c491-94e1-443c-dda9-5426db73b8a5"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
