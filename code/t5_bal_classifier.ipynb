{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "00805af9",
      "metadata": {
        "id": "00805af9"
      },
      "source": [
        "Install the packages required\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f42ea37d",
      "metadata": {
        "id": "f42ea37d"
      },
      "outputs": [],
      "source": [
        "!pip install pydot --quiet\n",
        "!pip install transformers --quiet\n",
        "!pip install sentencepiece --quiet\n",
        "!pip install nltk --quiet\n",
        "!pip install torch --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a74e61e8",
      "metadata": {
        "id": "a74e61e8"
      },
      "outputs": [],
      "source": [
        "# standard\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "import sklearn as sk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import os\n",
        "import nltk\n",
        "from nltk.data import find\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import re\n",
        "import string\n",
        "import seaborn as sns\n",
        "import os\n",
        "import textwrap\n",
        "\n",
        "\n",
        "import copy\n",
        "\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, \n",
        "    f1_score, \n",
        "    classification_report\n",
        ")\n",
        "\n",
        "import transformers\n",
        "from transformers import (\n",
        "    T5Tokenizer, \n",
        "    T5Model,\n",
        "    T5ForConditionalGeneration,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d711409d",
      "metadata": {
        "id": "d711409d"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv(\"https://adamhyman-public.s3.amazonaws.com/w266/for_modeling/augmented_and_balanced/train_data_balanced.csv\")\n",
        "\n",
        "df_valid = pd.read_csv(\"https://adamhyman-public.s3.amazonaws.com/w266/for_modeling/augmented_and_balanced/validation_data_balanced.csv\")\n",
        "\n",
        "df_test = pd.read_csv(\"https://adamhyman-public.s3.amazonaws.com/w266/for_modeling/test_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07f8cb56",
      "metadata": {
        "id": "07f8cb56"
      },
      "outputs": [],
      "source": [
        "df_train = df_train.astype({'toxic':'int', 'severe_toxic':'int', 'obscene':'int', 'threat':'int', 'insult':'int', 'identity_hate':'int'})\n",
        "#There are 3 null comments in train dataset, 1 in valid and 1 in test. They need to be removed or we get error.\n",
        "df_train = df_train.dropna(how='any',axis=0).reset_index(drop=True)\n",
        "df_valid = df_valid.dropna(how='any',axis=0).reset_index(drop=True)\n",
        "df_test = df_test.dropna(how='any',axis=0) .reset_index(drop=True)\n",
        "\n",
        "\n",
        "df_train = df_train[df_train[\"comment_text\"] != '']\n",
        "df_valid = df_valid[df_valid[\"comment_text\"] != '']\n",
        "df_test = df_test[df_test[\"comment_text\"] != '']\n",
        "\n",
        "df_train = df_train.astype({'toxic':'int', 'severe_toxic':'int', 'obscene':'int', 'threat':'int', 'insult':'int', 'identity_hate':'int'})\n",
        "df_valid = df_valid.astype({'toxic':'int', 'severe_toxic':'int', 'obscene':'int', 'threat':'int', 'insult':'int', 'identity_hate':'int'})\n",
        "df_test = df_test.astype({'toxic':'int', 'severe_toxic':'int', 'obscene':'int', 'threat':'int', 'insult':'int', 'identity_hate':'int'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a901e87f",
      "metadata": {
        "id": "a901e87f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "8bba3986-3485-42ce-94f4-a412e3071377"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        id  comment_text  toxic  severe_toxic  obscene  threat  insult  \\\n",
              "0     True          True  False         False    False   False   False   \n",
              "1     True          True  False         False    False   False   False   \n",
              "2     True          True  False         False    False   False   False   \n",
              "3     True          True  False         False    False   False   False   \n",
              "4     True          True  False         False    False   False   False   \n",
              "...    ...           ...    ...           ...      ...     ...     ...   \n",
              "3277  True          True  False         False    False   False   False   \n",
              "3278  True          True  False         False    False   False   False   \n",
              "3279  True          True  False         False    False   False   False   \n",
              "3280  True          True  False         False    False   False   False   \n",
              "3281  True          True  False         False    False   False   False   \n",
              "\n",
              "      identity_hate  text  \n",
              "0             False  True  \n",
              "1             False  True  \n",
              "2             False  True  \n",
              "3             False  True  \n",
              "4             False  True  \n",
              "...             ...   ...  \n",
              "3277          False  True  \n",
              "3278          False  True  \n",
              "3279          False  True  \n",
              "3280          False  True  \n",
              "3281          False  True  \n",
              "\n",
              "[3282 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-da097f41-1030-401e-bbaa-575a2d01ebb4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3277</th>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3278</th>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3279</th>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3280</th>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3281</th>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3282 rows × 9 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-da097f41-1030-401e-bbaa-575a2d01ebb4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-da097f41-1030-401e-bbaa-575a2d01ebb4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-da097f41-1030-401e-bbaa-575a2d01ebb4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "def  clean_text(text):\n",
        "    text =  text.lower()\n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\n",
        "    text = re.sub(r\"he's\", \"he is\", text)\n",
        "    text = re.sub(r\"she's\", \"she is\", text)\n",
        "    text = re.sub(r\"it's\", \"it is\", text)\n",
        "    text = re.sub(r\"that's\", \"that is\", text)\n",
        "    text = re.sub(r\"what's\", \"that is\", text)\n",
        "    text = re.sub(r\"where's\", \"where is\", text)\n",
        "    text = re.sub(r\"how's\", \"how is\", text)\n",
        "    text = re.sub(r\"'ll\", \" will\", text)\n",
        "    text = re.sub(r\"'ve\", \" have\", text)\n",
        "    text = re.sub(r\"'re\", \" are\", text)\n",
        "    text = re.sub(r\"'d\", \" would\", text)\n",
        "    text = re.sub(r\"'re\", \" are\", text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"can't\", \"cannot\", text)\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "    text = re.sub(r\"n'\", \"ng\", text)\n",
        "    text = re.sub(r\"'bout\", \"about\", text)\n",
        "    text = re.sub(r\"'til\", \"until\", text)\n",
        "    text = re.sub(r\"[-()\\\"\\#/@;:{}`+=~|!?,]\", \"\", text)\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation)) \n",
        "    text = re.sub(\"(\\W)\",\" \",text) \n",
        "\n",
        "    return text\n",
        "df_train[\"text\"] = df_train['comment_text'].apply(lambda text: clean_text(text))\n",
        "df_valid[\"text\"] = df_valid['comment_text'].apply(lambda text: clean_text(text))\n",
        "df_test[\"text\"] = df_test['comment_text'].apply(lambda text: clean_text(text))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ade9f8c",
      "metadata": {
        "id": "3ade9f8c"
      },
      "outputs": [],
      "source": [
        "train_comments, train_labels = df_train[\"text\"], df_train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n",
        "valid_comments, valid_labels = df_valid[\"text\"], df_valid[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n",
        "test_comments, test_labels = df_test[\"text\"], df_test[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e82f497",
      "metadata": {
        "id": "1e82f497"
      },
      "outputs": [],
      "source": [
        "def get_labels(df):\n",
        "    labels_li = [' '.join(x.lower().split()) for x in df.columns.to_list()[:6]]\n",
        "    labels_matrix = np.array([labels_li] * len(df))\n",
        "\n",
        "    mask = df.iloc[:, :6].values.astype(bool)\n",
        "    labels = []\n",
        "    for l, m in zip(labels_matrix, mask):\n",
        "        x = l[m]\n",
        "        if len(x) > 0:\n",
        "            labels.append(' , '.join(x.tolist()) + ' </s>')\n",
        "        else:\n",
        "            labels.append('none </s>')\n",
        "    return labels\n",
        "\n",
        "def get_texts(df):\n",
        "    texts = 'multilabel classification: ' + df\n",
        "    \n",
        "    return texts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iUaE_5USP7Mo",
      "metadata": {
        "id": "iUaE_5USP7Mo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83350350-8737-4126-b865-301b62b725ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "multilabel classification: lead paragraph the lead does not refers that the little girl and her siblings were left unattended in an unlocked ground floor hotel room while the parents went to a restaurant any reader will think that she disappeared while staying with her parents in the apartment and this was not what really happened\n",
            "['none </s>', 'none </s>', 'none </s>', 'none </s>', 'none </s>']\n",
            "       toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
            "0          0             0        0       0       0              0\n",
            "1          0             0        0       0       0              0\n",
            "2          0             0        0       0       0              0\n",
            "3          0             0        0       0       0              0\n",
            "4          0             0        0       0       0              0\n",
            "...      ...           ...      ...     ...     ...            ...\n",
            "13126      1             0        1       1       0              0\n",
            "13127      1             0        0       1       1              0\n",
            "13128      0             0        0       0       0              0\n",
            "13129      0             0        0       0       0              0\n",
            "13130      0             0        0       0       0              0\n",
            "\n",
            "[13131 rows x 6 columns]\n",
            "TEXT -\tmultilabel classification:  have you seen how incompetent middle easterners are their schools are so much worse than ours  the middle easterners are animals they kill another human for no reason  those middle easterners have been invaded so many times they have been brainwashed  arabs are ignorant and their culture is based on violence and war  middle easterners dont appreciate western culture and their western allies like we do  the only thing the middle east has accomplished is making shirts with holes in them \n",
            "LABEL -\ttoxic , identity_hate </s>\n",
            "\n",
            "TEXT -\tmultilabel classification: agreed removed   \n",
            "LABEL -\tnone </s>\n",
            "\n",
            "TEXT -\tmultilabel classification:    let me add a comment here in this discussion  while it certainly seems strange that jokes are included on a page concerning  areligiong dismissing it completely without some discussion and consideration would be doing a disservice  it may very well be that bektashi culture is intimately tied to a rich collection of jokes which serve some pedagogical role in the religion  for example if we look at this site  httpwwwalevibektasiorgxalevis1htm  it is written that  x alevibektashi humor alevis and bektashis love to use humor to mock religious legalism hypocrisy and those who they feel have persecuted them this humor reveals much about alevibektashi thought to understand their humor is to understand the alevibektashis  approach to life religion and their place in society here are only a few examples of the hundreds of bektashi jokes fä kralar that have been collected this first joke reflects the alevi attitude toward the legalism of the five pillars   the author is john shindeldecker and he is listed as one of the researchers on the fairly officiallooking homepage   httpwwwalevibektasiorgmakaleenglhtm  in this case this material suggests that humor is an integral part of defining bektashi culture as distinctive from other factions of islam  much as zen buddhism is characterized by the sometimes meaningless koans or gospel music in certain factions of protestant christianity  in all of these cases i do not think it is fair to say that it is like adding black jokes to an article about african americans  this comparison is also a bit overreaching and unwarranted  aldirmagonul has made a good faith case for why the jokes should be placed on this page  freestylefrappe has deleted it by only saying that it is suspect and racist  but given what we read from another source this seems an overly simplified and inaccurate depiction of the jokes section  i would like to hear an argument against these lines of evidence from freestylefrappe \n",
            "LABEL -\tnone </s>\n",
            "\n",
            "TEXT -\tmultilabel classification:   when they started working on shenmue not one person on that team said lets make this a killer app this is my reasoning behind removing that phrase from the article i do not see how this is a problem  if you can find a quote from someone involved in making the game that includes killer application  then i will quit\n",
            "LABEL -\tnone </s>\n",
            "\n",
            "TEXT -\tmultilabel classification:  binksternet and you did not even read the talk section to discuss this you simply reverted and told me to discuss this i already have you have not presented arguments against  \n",
            "LABEL -\tnone </s>\n",
            "\n",
            "TEXT -\tmultilabel classification: you are being incredibly dishonest by arguing it is not only mainstream among some groups it is only among cultural and physical anthropologists everywhere else it is not so stop beating around the bush already and present your version of the sentence\n",
            "LABEL -\tnone </s>\n",
            "\n",
            "val TEXT -\tmultilabel classification: just wait  your death is near\n",
            "val LABEL -\ttoxic , threat </s>\n",
            "\n",
            "val TEXT -\tmultilabel classification: al wistert was inducted into the eagles honor roll yesterday  i do not have all the details but someone should write this in\n",
            "val LABEL -\tnone </s>\n",
            "\n",
            "val TEXT -\tmultilabel classification: come on fag boy block me\n",
            "val LABEL -\ttoxic , insult </s>\n",
            "\n",
            "val TEXT -\tmultilabel classification: you motherfuckin bitch i want to rape you smelly whore stop blocking my account or get sick my niggas came home and kidnapped and raped you and your family\n",
            "val LABEL -\ttoxic , severe_toxic , obscene , threat , insult , identity_hate </s>\n",
            "\n",
            "val TEXT -\tmultilabel classification: shit cunt fuck nigger chink\n",
            "val LABEL -\ttoxic , severe_toxic , obscene , insult , identity_hate </s>\n",
            "\n",
            "val TEXT -\tmultilabel classification: i am going to be honest with you i am not going to pass your articles as ga because frankly they suck but that does not mean an agreement cannot be reached upon where your articles could still potentially reach ga  first thing you need to know is that good article status is a crock of shit the criteria for it is a fair model and it is not the problem the problem is that it is not governed by a committee any bozo like yourself is allowed to review an article and ultimately decide if the article in their mind merits inclusion into cgd club green dot the bar sits firmly in the mud snakes slither across it my articles on the other hand never have enjoyed the benefits of exception with extreme prejudice they have been held to the highest possible standard set by the criteria  now we get to your ga review of howling bells which i would like a full explanation for that is one of the agreements that i have set forth in order for you to avoid failure of your articles by me you can decide if you would like to abide by this resolution or not tell me something are you on drugs prescription or otherwise if that offends you i apologize but your review made me think of three things either somebody took a shit on your face and you could not see through it you had a big d in your ass or you were in a state of incoherence brought on by substance abuse i must confess i am ashamed that i am even having to talk to some punk ass high school kid who listens to crappy music not that i feel that i hold a higher place in this world than you because lord knows that is not the case it is just that i am sitting here having to deal with some goddamn little kid over the computer even at your age i know you understand where i am coming from  theres not much of a reward getting an article to ga status aside from that dot a small part of me would like to see the article at ga but it really does not concern me a whole lot whether it gets there or not howling bells is at ga and that is basically good enough for me as far as halfway gone goes at this point it is outta my hands you are just gonna have to accept the fact the article could possibly be delisted consider that your penalty for fucking around with other peoples hard work  ok so there are two agreements i have come up with that i think are very acceptable  a full explanation of your ga review of howling bells album i will guide you through it on the review page if you decide to play ball good on ya you must say the following to me here on my talk page i  am just a punk ass teenager who loves justin beiber loves the crappy music of justin beiber and loves crappy music from many other crappy artists the font has to be huge too none of this small crap also see if you can do it in some kind of color that would be cool  now i will not pass your articles if you agree to this but i will not fail them instead i will explain on the review pages that any reviewing of the articles that i have done up until this point should be disregarded my reasoning will be because of my inexperience as a first time reviewer has led me to realize that i am not capable for such a task i will wash my hands of the articles take them off hold and request that another editor undertake the review if you are unwilling to accept these agreements than you leave me no other choice but to expeditiously fail the articles let me know what you wanna do kid  meow\n",
            "val LABEL -\ttoxic </s>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "texts = get_texts(train_comments)\n",
        "labels = get_labels(train_labels)\n",
        "\n",
        "\n",
        "for text, label in zip(texts[854:860], labels[854:860]):\n",
        "     print(f'TEXT -\\t{text}')\n",
        "     print(f'LABEL -\\t{label}')\n",
        "     print()\n",
        "\n",
        "val_texts = get_texts(valid_comments)\n",
        "val_labels = get_labels(valid_labels)\n",
        "\n",
        "\n",
        "for text, label in zip(val_texts[854:860], val_labels[854:860]):\n",
        "     print(f'val TEXT -\\t{text}')\n",
        "     print(f'val LABEL -\\t{label}')\n",
        "     print()\n",
        "# print(type(train_comments))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee7a8697",
      "metadata": {
        "id": "ee7a8697",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "outputId": "5337da18-651b-4be9-d77a-bb9862f6f493"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "categories = df_train.columns.to_list()[3:8]\n",
        "plt.figure(figsize=(6, 4))\n",
        "print(categories)\n",
        "\n",
        "ax = sns.barplot(x=categories, y=df_train.iloc[:, 3:8].sum().values)\n",
        "plt.ylabel('Total ')\n",
        "plt.xlabel('Toxicity ')\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d95f6fc7",
      "metadata": {
        "id": "d95f6fc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "outputId": "5c38caf0-563a-416c-e2af-b974e5d6b4d1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# no of samples having multiple labels\n",
        "row_sums = df_train.iloc[:, 3:8].sum(axis=1)\n",
        "multilabel_counts = row_sums.value_counts()\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "ax = sns.barplot(x=multilabel_counts.index, y=multilabel_counts.values)\n",
        "plt.ylabel('Total samples')\n",
        "plt.xlabel('Number of labels')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46692d94",
      "metadata": {
        "id": "46692d94"
      },
      "source": [
        "Feed the large T5 dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "254bb3b1",
      "metadata": {
        "id": "254bb3b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f424440-bd1a-4904-e7b8-80bf8fff7ddc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "class Config:\n",
        "    def __init__(self):\n",
        "        super(Config, self).__init__()\n",
        "\n",
        "        self.SEED = 42\n",
        "        self.MODEL_PATH = 't5-base'\n",
        "\n",
        "        # data\n",
        "        self.TOKENIZER = T5Tokenizer.from_pretrained(self.MODEL_PATH)\n",
        "        self.SRC_MAX_LENGTH = 320\n",
        "        self.TGT_MAX_LENGTH = 20\n",
        "        self.BATCH_SIZE = 16\n",
        "        self.VALIDATION_SPLIT = 0.25\n",
        "\n",
        "        # model\n",
        "        self.DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.FULL_FINETUNING = True\n",
        "        self.LR = 3e-5\n",
        "        self.OPTIMIZER = 'AdamW'\n",
        "        self.CRITERION = 'BCEWithLogitsLoss'\n",
        "        self.SAVE_BEST_ONLY = True\n",
        "        self.N_VALIDATE_DUR_TRAIN = 3\n",
        "        self.EPOCHS = 2\n",
        "\n",
        "config = Config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddb6fc9c",
      "metadata": {
        "id": "ddb6fc9c"
      },
      "outputs": [],
      "source": [
        "class T5Dataset(Dataset):\n",
        "    def __init__(self, df, texts, labels, indices, set_type=None):\n",
        "        super(T5Dataset, self).__init__()\n",
        "\n",
        "        df = df.iloc[indices]\n",
        "        self.texts = get_texts(texts)\n",
        "        self.set_type = set_type\n",
        "        if self.set_type != 'test':\n",
        "            self.labels = get_labels(labels)\n",
        "\n",
        "        self.tokenizer = config.TOKENIZER\n",
        "        self.src_max_length = config.SRC_MAX_LENGTH\n",
        "        self.tgt_max_length = config.TGT_MAX_LENGTH\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        src_tokenized = self.tokenizer.encode_plus(\n",
        "            self.texts[index], \n",
        "            max_length=self.src_max_length,\n",
        "            pad_to_max_length=True,\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_token_type_ids=False,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        src_input_ids = src_tokenized['input_ids'].squeeze()\n",
        "        src_attention_mask = src_tokenized['attention_mask'].squeeze()\n",
        "\n",
        "        if self.set_type != 'test':\n",
        "            tgt_tokenized = self.tokenizer.encode_plus(\n",
        "                self.labels[index], \n",
        "                max_length=self.tgt_max_length,\n",
        "                pad_to_max_length=True,\n",
        "                truncation=True,\n",
        "                return_attention_mask=True,\n",
        "                return_token_type_ids=False,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "            tgt_input_ids = tgt_tokenized['input_ids'].squeeze()\n",
        "            tgt_attention_mask = tgt_tokenized['attention_mask'].squeeze()\n",
        "\n",
        "            return {\n",
        "                'src_input_ids': src_input_ids.long(),\n",
        "                'src_attention_mask': src_attention_mask.long(),\n",
        "                'tgt_input_ids': tgt_input_ids.long(),\n",
        "                'tgt_attention_mask': tgt_attention_mask.long()\n",
        "            }\n",
        "\n",
        "        return {\n",
        "            'src_input_ids': src_input_ids.long(),\n",
        "            'src_attention_mask': src_attention_mask.long()\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74f2ed1f",
      "metadata": {
        "id": "74f2ed1f"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_data = T5Dataset(df_train, train_comments, train_labels, len(train_comments)-1)\n",
        "\n",
        "val_data = T5Dataset(df_valid, valid_comments, valid_labels, len(valid_comments)-1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(train_data, batch_size=config.BATCH_SIZE)\n",
        "val_dataloader = DataLoader(val_data, batch_size=config.BATCH_SIZE)\n",
        "print(val_dataloader)\n",
        "b = next(iter(train_dataloader))\n",
        "for k, v in b.items():\n",
        "    print(f'{k} shape: {v.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnkp-KfVNJVD",
        "outputId": "2687f39e-613f-4f77-d3d5-74d5e1a1ee7a"
      },
      "id": "lnkp-KfVNJVD",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<torch.utils.data.dataloader.DataLoader object at 0x7f5b9f59aa30>\n",
            "src_input_ids shape: torch.Size([16, 320])\n",
            "src_attention_mask shape: torch.Size([16, 320])\n",
            "tgt_input_ids shape: torch.Size([16, 20])\n",
            "tgt_attention_mask shape: torch.Size([16, 20])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/models/t5/tokenization_t5.py:226: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16cdad92",
      "metadata": {
        "id": "16cdad92"
      },
      "outputs": [],
      "source": [
        "class T5Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(T5Model, self).__init__()\n",
        "\n",
        "        self.t5_model = T5ForConditionalGeneration.from_pretrained(config.MODEL_PATH)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids, \n",
        "        attention_mask=None, \n",
        "        decoder_input_ids=None, \n",
        "        decoder_attention_mask=None, \n",
        "        labels=None\n",
        "        ):\n",
        "\n",
        "        return self.t5_model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            decoder_input_ids=decoder_input_ids,\n",
        "            decoder_attention_mask=decoder_attention_mask,\n",
        "            labels=labels,\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11dfe94f",
      "metadata": {
        "id": "11dfe94f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1afa537-4e6c-4256-dbeb-995fc51c9f7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "print(config.DEVICE)\n",
        "device = config.DEVICE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ef5091c",
      "metadata": {
        "id": "5ef5091c"
      },
      "outputs": [],
      "source": [
        "def get_ohe(x):\n",
        "    labels_li = ['_'.join(x.lower().split()) for x in df_train.columns.to_list()[3:8]]\n",
        "    labels_li_indices = dict()\n",
        "    for idx, label in enumerate(labels_li):\n",
        "        labels_li_indices[label] = idx\n",
        "        \n",
        "    y = [labels.split(', ') for labels in x]\n",
        "    ohe = []\n",
        "    for labels in y:\n",
        "        temp = [0] * 6\n",
        "        for label in labels:\n",
        "            idx = labels_li_indices.get(label, -1)\n",
        "            if idx != -1:\n",
        "                temp[idx] = 1\n",
        "        ohe.append(temp)\n",
        "    ohe = np.array(ohe)\n",
        "    return ohe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cc3fb8c",
      "metadata": {
        "id": "8cc3fb8c"
      },
      "outputs": [],
      "source": [
        "def val(model, val_dataloader, criterion):\n",
        "    \n",
        "    val_loss = 0\n",
        "    true, pred = [], []\n",
        "    \n",
        "    # set model.eval() every time during evaluation\n",
        "    model.eval()\n",
        "    \n",
        "    for step, batch in enumerate(val_dataloader):\n",
        "        # unpack the batch contents and push them to the device (cuda or cpu).\n",
        "        b_src_input_ids = batch['src_input_ids'].to(device)\n",
        "        b_src_attention_mask = batch['src_attention_mask'].to(device)\n",
        "    \n",
        "        b_tgt_input_ids = batch['tgt_input_ids']\n",
        "        labels = b_tgt_input_ids.to(device)\n",
        "        labels[labels[:, :] == config.TOKENIZER.pad_token_id] = -100\n",
        "\n",
        "        b_tgt_attention_mask = batch['tgt_attention_mask'].to(device)\n",
        "\n",
        "        # using torch.no_grad() during validation/inference is faster -\n",
        "        # - since it does not update gradients.\n",
        "        with torch.no_grad():\n",
        "            # forward pass\n",
        "            outputs = model(\n",
        "                input_ids=b_src_input_ids, \n",
        "                attention_mask=b_src_attention_mask,\n",
        "                labels=labels,\n",
        "                decoder_attention_mask=b_tgt_attention_mask)\n",
        "            loss = outputs[0]\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # get true \n",
        "            for true_id in b_tgt_input_ids:\n",
        "                true_decoded = config.TOKENIZER.decode(true_id)\n",
        "                true.append(true_decoded)\n",
        "\n",
        "            # get pred (decoder generated textual label ids)\n",
        "            pred_ids = model.t5_model.generate(\n",
        "                input_ids=b_src_input_ids, \n",
        "                attention_mask=b_src_attention_mask\n",
        "            )\n",
        "            pred_ids = pred_ids.cpu().numpy()\n",
        "            for pred_id in pred_ids:\n",
        "                pred_decoded = config.TOKENIZER.decode(pred_id)\n",
        "                pred.append(pred_decoded)\n",
        "\n",
        "    true_ohe = get_ohe(true)\n",
        "    pred_ohe = get_ohe(pred)\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_dataloader)\n",
        "    print('Val loss:', avg_val_loss)\n",
        "    print('Val accuracy:', accuracy_score(true_ohe, pred_ohe))\n",
        "\n",
        "    val_micro_f1_score = f1_score(true_ohe, pred_ohe, average='micro')\n",
        "    print('Val micro f1 score:', val_micro_f1_score)\n",
        "    return val_micro_f1_score\n",
        "\n",
        "\n",
        "def train(\n",
        "    model,  \n",
        "    train_dataloader, \n",
        "    val_dataloader, \n",
        "    criterion, \n",
        "    optimizer, \n",
        "    scheduler, \n",
        "    epoch\n",
        "    ):\n",
        "    \n",
        "    # we validate config.N_VALIDATE_DUR_TRAIN times during the training loop\n",
        "    nv = config.N_VALIDATE_DUR_TRAIN\n",
        "    temp = len(train_dataloader) // nv\n",
        "    temp = temp - (temp % 100)\n",
        "    validate_at_steps = [temp * x for x in range(1, nv + 1)]\n",
        "    \n",
        "    train_loss = 0\n",
        "    for step, batch in enumerate(tqdm(train_dataloader, \n",
        "                                      desc='Epoch ' + str(epoch))):\n",
        "        # set model.eval() every time during training\n",
        "        model.train()\n",
        "        \n",
        "        # unpack the batch contents and push them to the device (cuda or cpu).\n",
        "        b_src_input_ids = batch['src_input_ids'].to(device)\n",
        "        b_src_attention_mask = batch['src_attention_mask'].to(device)\n",
        "    \n",
        "        labels = batch['tgt_input_ids'].to(device)\n",
        "        labels[labels[:, :] == config.TOKENIZER.pad_token_id] = -100\n",
        "\n",
        "        b_tgt_attention_mask = batch['tgt_attention_mask'].to(device)\n",
        "\n",
        "        # clear accumulated gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward pass\n",
        "        outputs = model(input_ids=b_src_input_ids, \n",
        "                        attention_mask=b_src_attention_mask,\n",
        "                        labels=labels,\n",
        "                        decoder_attention_mask=b_tgt_attention_mask)\n",
        "        loss = outputs[0]\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # update weights\n",
        "        optimizer.step()\n",
        "        \n",
        "        # update scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        if step in validate_at_steps:\n",
        "            print(f'-- Step: {step}')\n",
        "            _ = val(model, val_dataloader, criterion)\n",
        "    \n",
        "    avg_train_loss = train_loss / len(train_dataloader)\n",
        "    print('Training loss:', avg_train_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8f6a95d",
      "metadata": {
        "id": "c8f6a95d"
      },
      "outputs": [],
      "source": [
        "def run():\n",
        "    # setting a seed ensures reproducible results.\n",
        "    # seed may affect the performance too.\n",
        "    torch.manual_seed(config.SEED)\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    \n",
        "    # define the parameters to be optmized -\n",
        "    # - and add regularization\n",
        "    if config.FULL_FINETUNING:\n",
        "        param_optimizer = list(model.named_parameters())\n",
        "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "        optimizer_parameters = [\n",
        "            {\n",
        "                \"params\": [\n",
        "                    p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
        "                ],\n",
        "                \"weight_decay\": 0.001,\n",
        "            },\n",
        "            {\n",
        "                \"params\": [\n",
        "                    p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
        "                ],\n",
        "                \"weight_decay\": 0.0,\n",
        "            },\n",
        "        ]\n",
        "        optimizer = optim.AdamW(optimizer_parameters, lr=config.LR)\n",
        "\n",
        "    num_training_steps = len(train_dataloader) * config.EPOCHS\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=num_training_steps\n",
        "    )\n",
        "\n",
        "    max_val_micro_f1_score = float('-inf')\n",
        "    for epoch in range(config.EPOCHS):\n",
        "        train(model, train_dataloader, val_dataloader, criterion, optimizer, scheduler, epoch)\n",
        "        val_micro_f1_score = val(model, val_dataloader, criterion)\n",
        "\n",
        "        if config.SAVE_BEST_ONLY:\n",
        "            if val_micro_f1_score > max_val_micro_f1_score:\n",
        "                best_model = copy.deepcopy(model)\n",
        "                best_val_micro_f1_score = val_micro_f1_score\n",
        "\n",
        "                model_name = 't5_best_model'\n",
        "                torch.save(best_model.state_dict(), model_name + '.pt')\n",
        "\n",
        "                print(f'--- Best Model. Val loss: {max_val_micro_f1_score} -> {val_micro_f1_score}')\n",
        "                max_val_micro_f1_score = val_micro_f1_score\n",
        "\n",
        "    return best_model, best_val_micro_f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77df35d0",
      "metadata": {
        "id": "77df35d0"
      },
      "outputs": [],
      "source": [
        "model = T5Model()\n",
        "model.to(device);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6ab0db3",
      "metadata": {
        "id": "c6ab0db3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ffc9b63-395a-4a04-8748-e92fdd919ed5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0:  24%|██▍       | 200/821 [01:00<03:08,  3.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Step: 200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1288: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "Epoch 0:  24%|██▍       | 201/821 [02:38<5:04:48, 29.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss: 0.28420481064076564\n",
            "Val accuracy: 0.6264472882388787\n",
            "Val micro f1 score: 0.5150602409638555\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/models/t5/tokenization_t5.py:226: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
            "  warnings.warn(\n",
            "Epoch 0:  49%|████▊     | 400/821 [03:37<02:05,  3.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Step: 400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1288: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "Epoch 0:  49%|████▉     | 401/821 [05:17<3:30:59, 30.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss: 0.20378836665700362\n",
            "Val accuracy: 0.7516758074344911\n",
            "Val micro f1 score: 0.6271050521251003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/models/t5/tokenization_t5.py:226: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
            "  warnings.warn(\n",
            "Epoch 0:  73%|███████▎  | 600/821 [06:17<01:06,  3.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Step: 600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1288: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "Epoch 0:  73%|███████▎  | 601/821 [07:58<1:51:44, 30.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss: 0.19151277597812774\n",
            "Val accuracy: 0.7730042656916515\n",
            "Val micro f1 score: 0.6635794061563607\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/models/t5/tokenization_t5.py:226: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
            "  warnings.warn(\n",
            "Epoch 0: 100%|██████████| 821/821 [09:04<00:00,  1.51it/s]\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1288: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 0.5369848682659697\n",
            "Val loss: 0.1765579403495615\n",
            "Val accuracy: 0.7809262644728824\n",
            "Val micro f1 score: 0.6927932667017359\n",
            "--- Best Model. Val loss: -inf -> 0.6927932667017359\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1:   0%|          | 0/821 [00:00<?, ?it/s]/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/models/t5/tokenization_t5.py:226: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
            "  warnings.warn(\n",
            "Epoch 1:  24%|██▍       | 200/821 [00:59<03:05,  3.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Step: 200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1288: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "Epoch 1:  24%|██▍       | 201/821 [02:42<5:20:21, 31.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss: 0.17041441825336046\n",
            "Val accuracy: 0.7882388787324802\n",
            "Val micro f1 score: 0.694391244870041\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/models/t5/tokenization_t5.py:226: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
            "  warnings.warn(\n",
            "Epoch 1:  49%|████▊     | 400/821 [03:42<02:05,  3.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Step: 400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1288: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "Epoch 1:  49%|████▉     | 401/821 [05:26<3:40:32, 31.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss: 0.16163457254653127\n",
            "Val accuracy: 0.7928092626447288\n",
            "Val micro f1 score: 0.7162198035572074\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/models/t5/tokenization_t5.py:226: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
            "  warnings.warn(\n",
            "Epoch 1:  73%|███████▎  | 600/821 [06:26<01:06,  3.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Step: 600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1288: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "Epoch 1:  73%|███████▎  | 601/821 [08:09<1:54:23, 31.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss: 0.16069654641148534\n",
            "Val accuracy: 0.7931139549055454\n",
            "Val micro f1 score: 0.7120107962213226\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/models/t5/tokenization_t5.py:226: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
            "  warnings.warn(\n",
            "Epoch 1: 100%|██████████| 821/821 [09:15<00:00,  1.48it/s]\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1288: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 0.19907915718789948\n",
            "Val loss: 0.16052742875678447\n",
            "Val accuracy: 0.7940280316879951\n",
            "Val micro f1 score: 0.7168690958164643\n",
            "--- Best Model. Val loss: 0.6927932667017359 -> 0.7168690958164643\n"
          ]
        }
      ],
      "source": [
        "best_model, best_val_micro_f1_score = run()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = T5Dataset(df_test, test_comments, test_labels, len(test_comments)-1, set_type = 'test')\n",
        "\n",
        "test_dataloader = DataLoader(test_data, batch_size=config.BATCH_SIZE)\n"
      ],
      "metadata": {
        "id": "pPkQfIHACzaw"
      },
      "id": "pPkQfIHACzaw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model):\n",
        "    val_loss = 0\n",
        "    pred = []\n",
        "    model.eval()\n",
        "    for step, batch in tqdm(enumerate(test_dataloader), total=len(test_dataloader)):\n",
        "        b_src_input_ids = batch['src_input_ids'].to(device)\n",
        "        b_src_attention_mask = batch['src_attention_mask'].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # get pred\n",
        "            pred_ids = model.t5_model.generate(\n",
        "                input_ids=b_src_input_ids, \n",
        "                attention_mask=b_src_attention_mask\n",
        "            )\n",
        "            pred_ids = pred_ids.cpu().numpy()\n",
        "            for pred_id in pred_ids:\n",
        "                pred_decoded = config.TOKENIZER.decode(pred_id)\n",
        "                pred.append(pred_decoded)\n",
        "\n",
        "    pred_ohe = get_ohe(pred)\n",
        "    return pred_ohe\n",
        "\n",
        "pred_ohe = predict(best_model)"
      ],
      "metadata": {
        "id": "XDlDeKF_C1NX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04b71b70-406a-4a46-dbe4-45820a231285"
      },
      "id": "XDlDeKF_C1NX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/3999 [00:00<?, ?it/s]/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1288: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "100%|██████████| 3999/3999 [23:48<00:00,  2.80it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(best_val_micro_f1_score)\n",
        "print(pred_ohe)\n",
        "y_test = test_labels\n",
        "y_test_pred = np.where(pred_ohe>=0.5, 1, 0)\n",
        "y_test_pred[:10] "
      ],
      "metadata": {
        "id": "-v54_NfikLOV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8691f9b-9f94-4c88-deae-554ab8606bfa"
      },
      "id": "-v54_NfikLOV",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7168690958164643\n",
            "[[0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0]\n",
            " ...\n",
            " [0 0 0 0 0 0]\n",
            " [1 1 0 0 0 0]\n",
            " [0 0 0 0 0 0]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(pred_ohe)\n",
        "print(type(pred_ohe))"
      ],
      "metadata": {
        "id": "67tvZ5RbNDnO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b90472fc-948d-44ea-b826-f266c2e86b60"
      },
      "id": "67tvZ5RbNDnO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0]\n",
            " ...\n",
            " [0 0 0 0 0 0]\n",
            " [1 1 0 0 0 0]\n",
            " [0 0 0 0 0 0]]\n",
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, y_test_pred, target_names=['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'], zero_division = 0))"
      ],
      "metadata": {
        "id": "NbvztNQ1o9Y2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62fdbba9-0b91-433e-8cae-73ec69e5dd9f"
      },
      "id": "NbvztNQ1o9Y2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               precision    recall  f1-score   support\n",
            "\n",
            "        toxic       0.53      0.60      0.56      6090\n",
            " severe_toxic       0.04      0.96      0.08       367\n",
            "      obscene       0.52      0.06      0.10      3691\n",
            "       threat       0.00      0.00      0.00       211\n",
            "       insult       0.00      0.00      0.00      3427\n",
            "identity_hate       0.00      0.00      0.00       712\n",
            "\n",
            "    micro avg       0.26      0.29      0.28     14498\n",
            "    macro avg       0.18      0.27      0.12     14498\n",
            " weighted avg       0.36      0.29      0.26     14498\n",
            "  samples avg       0.03      0.03      0.03     14498\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import hamming_loss\n",
        "hamming_loss(y_test, y_test_pred)"
      ],
      "metadata": {
        "id": "-VWKPj7QpGVE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43e015f2-7b96-4455-d19a-e2bdc9ca7a51"
      },
      "id": "-VWKPj7QpGVE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.05739490193626378"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "roc_auc_score(y_test, y_test_pred, average=None)"
      ],
      "metadata": {
        "id": "vjhIltWH6afz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "055072d3-d19f-4df9-8fc3-e4fdfa31c277"
      },
      "id": "vjhIltWH6afz",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.77105279, 0.92196338, 0.52632976, 0.49753129, 0.5       ,\n",
              "       0.5       ])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "roc_auc_score(y_test, y_test_pred, average=\"macro\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pdwMOrAWw1-",
        "outputId": "fef6af89-8d16-4837-a91f-8142a2e835a7"
      },
      "id": "4pdwMOrAWw1-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6194795379675934"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}